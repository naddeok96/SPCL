#!/usr/bin/env python3
"""
generate_dataset.py

This script uses an evolutionary algorithm to search for good hyperparameter
actions in our curriculum learning environment (from curriculum_env.py) and saves
the full transition dataset to disk. Each candidate is a multi-phase candidate that
already contains actions for all phases. For a 3‚Äêphase curriculum, a candidate is a
15-dimensional vector (3 segments of 5 dimensions each) and produces 3 state-action-reward-next_state
transitions.
"""

import os
import yaml
import torch
import random
import numpy as np
import copy
from tqdm import tqdm

from curriculum_env import CurriculumEnv

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def load_config(config_file):
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    return config

def safe_normalize(arr):
    """
    Normalizes a non-negative array so that it sums to 1.
    If the sum is zero, returns a uniform distribution.
    Intended for normalizing the mixing ratio (indices 1-3 of each phase block).
    """
    arr = np.maximum(arr, 0)
    total = arr.sum()
    if total == 0:
        return np.ones_like(arr) / len(arr)
    return arr / total

def initialize_population(pop_size, candidate_dim, num_phases, macro_actions=None, lr_range=None):
    """
    Initializes a population of candidate actions.
    
    Each candidate is a vector of length candidate_dim = num_phases * 5.
    For each phase (block of 5 elements):
      - Element 0: A learning rate value in [lr_range[0], lr_range[1]].
      - Elements 1-3: Raw mixing ratios that are normalized to sum to 1.
      - Element 4: A sample usage fraction in [0, 1].
      
    If macro_actions are provided (as a dict of already processed candidate vectors),
    each candidate is generated by perturbing one of these macro actions.
    Otherwise, the candidate is built from scratch.
    """
    population = []
    unit = candidate_dim // num_phases  # should be 5
    if macro_actions:
        macro_vals = list(macro_actions.values())  # expected to be candidate vectors
        while len(population) < pop_size:
            base_action = np.array(random.choice(macro_vals))
            base_action = base_action.flatten()  # ensure 1-D
            # If the chosen macro action is smaller than candidate_dim, tile it.
            if base_action.shape[0] != candidate_dim:
                base_action = np.tile(base_action, num_phases)
            noise = np.random.normal(0, 0.1, size=candidate_dim)
            candidate = base_action + noise
            # Enforce constraints for each phase:
            for phase in range(num_phases):
                idx = phase * unit
                candidate[idx] = np.clip(candidate[idx], lr_range[0], lr_range[1])
                candidate[idx+1: idx+4] = safe_normalize(candidate[idx+1: idx+4])
                candidate[idx+4] = np.clip(candidate[idx+4], 0, 1)
            population.append(candidate)
    else:
        for _ in range(pop_size):
            cand = np.zeros(candidate_dim)
            for phase in range(num_phases):
                idx = phase * unit
                cand[idx] = random.uniform(lr_range[0], lr_range[1])
                cand[idx+1: idx+4] = safe_normalize(np.random.rand(3))
                cand[idx+4] = random.uniform(0, 1)
            population.append(cand)
    return population

def mutate(candidate, mutation_rate, candidate_dim, num_phases, lr_range):
    """
    Mutates a candidate by adding Gaussian noise and enforcing the per-phase constraints.
    """
    mutated = candidate + np.random.normal(0, mutation_rate, size=candidate.shape)
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        idx = phase * unit
        mutated[idx] = np.clip(mutated[idx], lr_range[0], lr_range[1])
        mutated[idx+1: idx+4] = safe_normalize(mutated[idx+1: idx+4])
        mutated[idx+4] = np.clip(mutated[idx+4], 0, 1)
    return mutated

def crossover(parent1, parent2, candidate_dim, num_phases, lr_range):
    """
    Creates a child candidate by crossing over parents and enforcing the constraints.
    """
    child = parent1.copy()
    for i in range(len(child)):
        if random.random() < 0.5:
            child[i] = parent2[i]
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        idx = phase * unit
        child[idx] = np.clip(child[idx], lr_range[0], lr_range[1])
        child[idx+1: idx+4] = safe_normalize(child[idx+1: idx+4])
        child[idx+4] = np.clip(child[idx+4], 0, 1)
    return child

def evaluate_candidate(env, candidate, candidate_dim, num_phases):
    """
    Evaluates a multi-phase candidate on the given environment.
    
    The candidate is split into num_phases segments (each of length 5).
    For each phase, the corresponding 5-dim action is applied to the environment
    (by calling env.step with that action). Each phase produces a transition:
      (state, action_phase, reward, next_state, done)
    The candidate's fitness is computed as the sum of rewards over all phases.
    
    Returns:
        transitions: list of transitions (one per phase)
        aggregated_reward: sum of rewards from all phases
        final_state: state after applying all phases (or when done=True)
        done: final done flag from the environment
    """
    transitions = []
    aggregated_reward = 0.0
    state = env.reset()
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        action = candidate[phase*unit:(phase+1)*unit]
        next_state, reward, done = env.step(action)
        transitions.append((state, action, reward, next_state, done))
        aggregated_reward += reward
        state = next_state
        if done:
            break
    return transitions, aggregated_reward, state, done

def evolutionary_algorithm(env, num_phases, pop_size, generations, top_k, mutation_rate,
                           macro_actions, candidate_dim, lr_range, save_path=None):
    """
    Runs the evolutionary algorithm using multi-phase candidates.
    
    Each candidate is evaluated over an entire curriculum episode (num_phases steps)
    and produces a set of transitions (one per phase). All transitions are added to the dataset.
    The candidate's fitness (aggregated reward) is used to guide evolution.
    """
    history = []
    population = initialize_population(pop_size, candidate_dim, num_phases, macro_actions, lr_range)
    for gen in tqdm(range(generations), desc="Generations"):
        candidates = []
        for candidate in tqdm(population, desc="Evaluating candidates", leave=False):
            # Use a deepcopy so that each candidate is evaluated from the same starting state.
            env_copy = copy.deepcopy(env)
            transitions, agg_reward, final_state, done = evaluate_candidate(env_copy, candidate, candidate_dim, num_phases)
            candidates.append((agg_reward, candidate, transitions, final_state, done))
            # Add every transition (phase) to the history.
            for t in transitions:
                history.append(t)
            tqdm.write(f" Candidate aggregated reward: {agg_reward:.2f}")
        candidates.sort(key=lambda x: x[0], reverse=True)
        selected = [cand[1] for cand in candidates[:top_k]]
        new_population = []
        while len(new_population) < pop_size:
            parent1, parent2 = random.sample(selected, 2)
            child = crossover(parent1, parent2, candidate_dim, num_phases, lr_range)
            child = mutate(child, mutation_rate, candidate_dim, num_phases, lr_range)
            new_population.append(child)
        population = new_population

        # Periodically save the dataset.
        if save_path is not None and ((gen + 1) % 1 == 0):
            states = np.array([h[0] for h in history])
            actions = np.array([h[1] for h in history])
            rewards = np.array([h[2] for h in history])
            next_states = np.array([h[3] for h in history])
            dones = np.array([h[4] for h in history])
            np.savez(save_path, states=states, actions=actions, rewards=rewards,
                     next_states=next_states, dones=dones)
            tqdm.write(f"Saved partial dataset to {save_path} at generation {gen + 1}")
    return history

def main():
    config = load_config("config.yaml")
    set_seed(42)
    env = CurriculumEnv(config)
    num_phases = config["curriculum"].get("max_phases", 3)
    unit = 5  # each phase action is 5-dim
    candidate_dim = num_phases * unit  # total candidate dimension
    lr_range = config["curriculum"]["learning_rate_range"]
    macro_actions = config["rl"].get("macro_actions")  # Should be provided as candidate vectors (5-dim or 15-dim). If 5-dim, they will be tiled.
    
    # Ensure the save directory exists.
    os.makedirs(config["paths"]["save_path"], exist_ok=True)
    dataset_path = os.path.join(config["paths"]["save_path"], "evolutionary_dataset.npz")
    
    history = evolutionary_algorithm(env,
                                     num_phases=num_phases,
                                     pop_size=100,
                                     generations=10,
                                     top_k=10,
                                     mutation_rate=0.1,
                                     macro_actions=macro_actions,
                                     candidate_dim=candidate_dim,
                                     lr_range=lr_range,
                                     save_path=dataset_path)
    print(f"Evolutionary algorithm generated {len(history)} transitions.")
    
    # Final save of the dataset.
    states = np.array([h[0] for h in history])
    actions = np.array([h[1] for h in history])
    rewards = np.array([h[2] for h in history])
    next_states = np.array([h[3] for h in history])
    dones = np.array([h[4] for h in history])
    np.savez(dataset_path, states=states, actions=actions, rewards=rewards,
             next_states=next_states, dones=dones)
    print(f"Saved evolutionary dataset to {dataset_path}")

if __name__ == "__main__":
    main()
