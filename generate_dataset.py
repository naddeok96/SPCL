#!/usr/bin/env python3
"""
generate_dataset.py

This script uses an evolutionary algorithm to search for good hyperparameter
actions in our curriculum learning environment (from curriculum_env.py) and saves
the full transition dataset to disk. Each candidate is a multi-phase candidate that
already contains actions for all phases. For a 3â€phase curriculum, a candidate is a
15-dimensional vector (3 segments of 5 dimensions each) and produces 3 state-action-reward-next_state
transitions.
"""

import os
import yaml
import torch
import random
import copy
import argparse
from tqdm import tqdm

from curriculum_env import CurriculumEnv

def set_seed(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def load_config(config_file):
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    return config

def safe_normalize(arr: torch.Tensor) -> torch.Tensor:
    """
    Normalizes a non-negative 1D tensor so that it sums to 1.
    If the sum is zero, returns a uniform distribution.
    """
    arr = torch.clamp(arr, min=0.0)
    total = arr.sum()
    if total.item() == 0.0:
        return torch.full_like(arr, 1.0 / arr.numel())
    return arr / total

def initialize_population(pop_size, candidate_dim, num_phases, macro_actions=None, lr_range=None):
    """
    Initializes a population of candidate actions.
    
    Each candidate is a vector of length candidate_dim = num_phases * 5.
    For each phase (block of 5 elements):
      - Element 0: A learning rate value in [lr_range[0], lr_range[1]].
      - Elements 1-3: Raw mixing ratios that are normalized to sum to 1.
      - Element 4: A sample usage fraction in [0, 1].
      
    If macro_actions are provided (as a dict of already processed candidate vectors),
    each candidate is generated by perturbing one of these macro actions.
    Otherwise, the candidate is built from scratch.
    """
    population = []
    unit = candidate_dim // num_phases  # should be 5
    if macro_actions:
        macro_vals = [torch.tensor(v, dtype=torch.float32).flatten() for v in macro_actions.values()]  # expected to be candidate vectors
        while len(population) < pop_size:
            base_action = random.choice(macro_vals).clone()
            base_action = base_action.flatten()  # ensure 1-D
            # If the chosen macro action is smaller than candidate_dim, tile it.
            if base_action.numel() != candidate_dim:
                base_action = base_action.repeat(num_phases)[:candidate_dim]
                
            noise = torch.randn(candidate_dim) * 0.1
            candidate = base_action + noise
            # Enforce constraints for each phase:
            for phase in range(num_phases):
                idx = phase * unit
                candidate[idx] = candidate[idx].clamp(lr_range[0], lr_range[1])
                candidate[idx+1: idx+4] = safe_normalize(candidate[idx+1: idx+4])
                candidate[idx+4] = candidate[idx+4].clamp(0.0, 1.0)
            population.append(candidate)
    else:
        for _ in range(pop_size):
            cand = torch.zeros(candidate_dim, dtype=torch.float32)
            for phase in range(num_phases):
                idx = phase * unit
                cand[idx] = random.uniform(lr_range[0], lr_range[1])
                cand[idx+1: idx+4] = safe_normalize(torch.rand(3))
                cand[idx+4] = random.uniform(0, 1)
            population.append(cand)
    return population

def mutate(candidate: torch.Tensor,
           mutation_rate: float,
           candidate_dim: int,
           num_phases: int,
           lr_range: list) -> torch.Tensor:
    """
    Adds Gaussian noise to a candidate and re-applies per-phase constraints.
    """
    mutated = candidate + torch.randn_like(candidate) * mutation_rate
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        idx = phase * unit
        mutated[idx] = mutated[idx].clamp(lr_range[0], lr_range[1])
        mutated[idx+1:idx+4] = safe_normalize(mutated[idx+1:idx+4])
        mutated[idx+4] = mutated[idx+4].clamp(0.0, 1.0)
    return mutated

def crossover(parent1: torch.Tensor,
              parent2: torch.Tensor,
              candidate_dim: int,
              num_phases: int,
              lr_range: list) -> torch.Tensor:
    """
    Single-point crossover between two parent tensors plus constraint enforcement.
    """
    mask = torch.rand(candidate_dim) < 0.5
    child = parent1.clone()
    child[mask] = parent2[mask]
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        idx = phase * unit
        child[idx] = child[idx].clamp(lr_range[0], lr_range[1])
        child[idx+1:idx+4] = safe_normalize(child[idx+1:idx+4])
        child[idx+4] = child[idx+4].clamp(0.0, 1.0)
    return child

def evaluate_candidate(env, candidate, candidate_dim, num_phases):
    """
    Runs one curriculum episode for a candidate and returns:
      - transitions: list of (state, action, reward, next_state, done)
      - aggregated_reward: float sum of rewards
      - final_state: torch.Tensor of last observation
      - done: bool
    """
    transitions = []
    aggregated_reward = 0.0
    state = torch.tensor(env.reset(), dtype=torch.float32)
    unit = candidate_dim // num_phases
    for phase in range(num_phases):
        action = candidate[phase*unit:(phase+1)*unit]
        next_state, reward, done = env.step(action)
        transitions.append((state, action, reward, next_state, done))
        aggregated_reward += reward
        state = next_state
        if done:
            break
        
    return transitions, aggregated_reward, state, done

def evolutionary_algorithm(env, num_phases, pop_size, generations, top_k, mutation_rate,
                           macro_actions, candidate_dim, lr_range, save_path=None):
    """
    Runs the evolutionary algorithm using multi-phase candidates.
    
    Each candidate is evaluated over an entire curriculum episode (num_phases steps)
    and produces a set of transitions (one per phase). All transitions are added to the dataset.
    The candidate's fitness (aggregated reward) is used to guide evolution.
    """
    history = []
    population = initialize_population(pop_size, candidate_dim, num_phases, macro_actions, lr_range)
    for gen in tqdm(range(generations), desc="Generations"):
        candidates = []
        for candidate in tqdm(population, desc="Evaluating candidates", leave=False):
            # Use a deepcopy so that each candidate is evaluated from the same starting state.
            env_copy = copy.deepcopy(env)
            transitions, agg_reward, final_state, done = evaluate_candidate(env_copy, candidate, candidate_dim, num_phases)
            candidates.append((agg_reward, candidate, transitions, final_state, done))
            # Add every transition (phase) to the history.
            for t in transitions:
                history.append(t)
            tqdm.write(f" Candidate aggregated reward: {agg_reward:.2f}")
        candidates.sort(key=lambda x: x[0], reverse=True)
        selected = [cand[1] for cand in candidates[:top_k]]
        new_population = []
        while len(new_population) < pop_size:
            parent1, parent2 = random.sample(selected, 2)
            child = crossover(parent1, parent2, candidate_dim, num_phases, lr_range)
            child = mutate(child, mutation_rate, candidate_dim, num_phases, lr_range)
            new_population.append(child)
        population = new_population

        # Periodically save the dataset.
        if save_path:
            states      = torch.stack([h[0] for h in history])
            actions     = torch.stack([h[1] for h in history])
            rewards     = torch.tensor([h[2] for h in history], dtype=torch.float32)
            next_states = torch.stack([h[3] for h in history])
            dones       = torch.tensor([h[4] for h in history], dtype=torch.bool)
            torch.save({
                'states':      states,
                'actions':     actions,
                'rewards':     rewards,
                'next_states': next_states,
                'dones':       dones
            }, save_path)
            tqdm.write(f"Saved dataset to {save_path} at generation {gen+1}")

    return history

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config", "-c",
        default="config.yaml",
        help="path to your config YAML"
    )
    args   = parser.parse_args()
    config = load_config(args.config)
    
    set_seed(42)
    env = CurriculumEnv(config)
    num_phases = config["curriculum"].get("max_phases", 3)
    unit = 5  # each phase action is 5-dim
    candidate_dim = num_phases * unit  # total candidate dimension
    lr_range = config["curriculum"]["learning_rate_range"]
    macro_actions = config["rl"].get("macro_actions")  # Should be provided as candidate vectors (5-dim or 15-dim). If 5-dim, they will be tiled.
    
    # Ensure the save directory exists.
    os.makedirs(config["paths"]["save_path"], exist_ok=True)
    dataset_path = os.path.join(config["paths"]["save_path"], "evolutionary_dataset.pt")
    
    history = evolutionary_algorithm(env,
                                     num_phases=num_phases,
                                     pop_size   = config["rl"].get("ea_pop_size", 100),
                                     generations= config["rl"].get("ea_generations", 10),
                                     top_k      = config["rl"].get("ea_top_k", 10),
                                     mutation_rate = config["rl"].get("ea_mutation_rate", 0.1),
                                     macro_actions=macro_actions,
                                     candidate_dim=candidate_dim,
                                     lr_range=lr_range,
                                     save_path=dataset_path)
    print(f"Evolutionary algorithm generated {len(history)} transitions.")
    
    # Pre-allocate Python lists
    states_list      = []
    actions_list     = []
    rewards_list     = []  # Python floats
    next_states_list = []
    dones_list       = []  # Python bools

    # Suppose `history` is your list of transitions:  (state, action, reward, next_state, done)
    for (s, a, r, ns, d) in history:
        states_list.append(s)
        actions_list.append(a)
        rewards_list.append(float(r))    # store as plain float
        next_states_list.append(ns)
        dones_list.append(bool(d))       # store as plain bool

    # Now convert once at the end:
    states      = torch.stack(states_list)                      # (N, state_dim)
    actions     = torch.stack(actions_list)                     # (N, action_dim)
    rewards     = torch.tensor(rewards_list, dtype=torch.float32)  # (N,)
    next_states = torch.stack(next_states_list)                 # (N, state_dim)
    dones       = torch.tensor(dones_list, dtype=torch.bool)    # (N,)

    torch.save({
        'states':      states,
        'actions':     actions,
        'rewards':     rewards,
        'next_states': next_states,
        'dones':       dones
    }, dataset_path)
    print(f"Saved final dataset to {dataset_path}")

if __name__ == "__main__":
    main()








