"""
off_policy_train.py

This script loads the evolutionary dataset generated by generate_dataset.py,
populates a replay buffer with the transitions, performs off-policy training
using the DDPG algorithm, logs training metrics, and then performs full evaluation
episodes periodically. At every 5% of training, all plots (losses, reward progression,
aggregated histograms, and a detailed evaluation episode figure) and the model checkpoint 
are saved to a separate checkpoint directory.
"""

import os
import yaml
import torch
import numpy as np
import random
import argparse
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from tqdm import trange, tqdm


from rl_agent import DDPGAgent
from replay_buffer import ReplayBuffer, PERBuffer
from curriculum_env import CurriculumEnv

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def load_config(config_file):
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    return config

# ----- Episode Plotting Functions (adapted from analyze_history.py) -----

def breakdown_state(state, num_bins):
    """
    Breaks down the state vector into its constituent parts.
    Assumes the following layout:
      - Indices 0:num_bins                  : Easy Correct Histogram
      - Indices num_bins:2*num_bins         : Easy Incorrect Histogram
      - Indices 2*num_bins:3*num_bins       : Medium Correct Histogram
      - Indices 3*num_bins:4*num_bins       : Medium Incorrect Histogram
      - Indices 4*num_bins:5*num_bins       : Hard Correct Histogram
      - Indices 5*num_bins:6*num_bins       : Hard Incorrect Histogram
      - Indices 6*num_bins:6*num_bins+3     : Relative dataset sizes (3 values)
      - Indices 6*num_bins+3:6*num_bins+5     : Extra state features (2 values)
    Total length = 6*num_bins + 5.
    """
    breakdown = {}
    breakdown['easy_correct_hist'] = state[0:num_bins]
    breakdown['easy_incorrect_hist'] = state[num_bins:2*num_bins]
    breakdown['medium_correct_hist'] = state[2*num_bins:3*num_bins]
    breakdown['medium_incorrect_hist'] = state[3*num_bins:4*num_bins]
    breakdown['hard_correct_hist'] = state[4*num_bins:5*num_bins]
    breakdown['hard_incorrect_hist'] = state[5*num_bins:6*num_bins]
    breakdown['relative_sizes'] = state[6*num_bins:6*num_bins+3]
    breakdown['extra'] = state[6*num_bins+3:6*num_bins+5]
    return breakdown

def breakdown_action(action):
    """
    Breaks down the 5-dimensional action vector into:
      - Learning rate: action[0]
      - Mixing ratios for (Easy, Medium, Hard): action[1:4]
      - Sample usage fraction: action[4]
    """
    breakdown = {}
    breakdown['learning_rate'] = action[0]
    breakdown['mixing_ratios'] = action[1:4]
    breakdown['sample_usage_fraction'] = action[4]
    return breakdown

def plot_episode_figure(episode, group_name, num_bins, output_dir):
    """
    For a given episode, creates a detailed figure with:
      - TOP BLOCK: For each phase (transition), a row of 4 subplots:
            * Column 0: Combined "Easy" loss histogram (green for correct, red for incorrect).
            * Column 1: Combined "Medium" loss histogram.
            * Column 2: Combined "Hard" loss histogram.
            * Column 3: State Info bar chart (relative sizes and extra features).
         Each row is annotated with that phase’s reward.
      - BOTTOM BLOCK: Aggregated evolution across phases with 4 subplots:
            1. Learning Rate evolution.
            2. Sample Usage evolution.
            3. Mixing Ratios as a stacked bar plot (one bar per phase).
            4. Reward evolution (with the last phase reward divided by 10).
    The resulting figure is saved to the output directory.
    """
    num_phases = len(episode['states'])
    
    fig = plt.figure(figsize=(20, num_phases * 3 + 3))
    
    # TOP BLOCK: grid for each phase.
    gs_top = gridspec.GridSpec(nrows=num_phases, ncols=4, top=0.95, bottom=0.55, 
                               wspace=0.4, hspace=0.6, height_ratios=[1] * num_phases)
    # BOTTOM BLOCK: 1 row, 4 columns.
    gs_bot = gridspec.GridSpec(nrows=1, ncols=4, top=0.5, bottom=0.05, wspace=0.5)
    
    # TOP BLOCK: For each phase, plot state breakdown.
    for i in range(num_phases):
        state = episode['states'][i]
        reward_phase = episode['rewards'][i]
        s_break = breakdown_state(state, num_bins)
        x = np.arange(num_bins)
        width = 0.4
        
        # Column 0: Easy losses.
        ax_easy = fig.add_subplot(gs_top[i, 0])
        ax_easy.bar(x - width/2, s_break['easy_correct_hist'], width, color='green', label='Correct')
        ax_easy.bar(x + width/2, s_break['easy_incorrect_hist'], width, color='red', label='Incorrect')
        if i == 0:
            ax_easy.set_title("Easy Loss Hist", fontsize=10)
        ax_easy.set_ylabel(f"P{i+1}\nR: {reward_phase:.2f}", fontsize=9)
        ax_easy.tick_params(axis='both', labelsize=8)
        if i == 0:
            ax_easy.legend(fontsize=8)
        
        # Column 1: Medium losses.
        ax_med = fig.add_subplot(gs_top[i, 1])
        ax_med.bar(x - width/2, s_break['medium_correct_hist'], width, color='green')
        ax_med.bar(x + width/2, s_break['medium_incorrect_hist'], width, color='red')
        if i == 0:
            ax_med.set_title("Medium Loss Hist", fontsize=10)
        ax_med.tick_params(axis='both', labelsize=8)
        
        # Column 2: Hard losses.
        ax_hard = fig.add_subplot(gs_top[i, 2])
        ax_hard.bar(x - width/2, s_break['hard_correct_hist'], width, color='green')
        ax_hard.bar(x + width/2, s_break['hard_incorrect_hist'], width, color='red')
        if i == 0:
            ax_hard.set_title("Hard Loss Hist", fontsize=10)
        ax_hard.tick_params(axis='both', labelsize=8)
        
        # Column 3: State Info.
        ax_info = fig.add_subplot(gs_top[i, 3])
        state_info = np.concatenate([s_break['relative_sizes'], s_break['extra']])
        colors_info = ['blue', 'orange', 'purple', 'cyan', 'magenta']
        ax_info.bar(np.arange(5), state_info, color=colors_info)
        ax_info.set_xticks(np.arange(5))
        ax_info.set_xticklabels(['Easy', 'Med', 'Hard', 'PhaseRatio', 'AvailRatio'], 
                                rotation=45, fontsize=8)
        if i == 0:
            ax_info.set_title("State Info", fontsize=10)
        ax_info.tick_params(axis='both', labelsize=8)
    
    # BOTTOM BLOCK: Aggregated evolution across phases.
    phases = np.arange(1, num_phases + 1)
    learning_rates = [episode['actions'][i][0] for i in range(num_phases)]
    sample_usage = [episode['actions'][i][4] for i in range(num_phases)]
    mixing_ratios = np.array([episode['actions'][i][1:4] for i in range(num_phases)])
    rewards_phase = [episode['rewards'][i] for i in range(num_phases)]
    adjusted_rewards = np.array(rewards_phase, dtype=float)
    if len(adjusted_rewards) > 0:
        adjusted_rewards[-1] = adjusted_rewards[-1] / 10.0

    ax_lr = fig.add_subplot(gs_bot[0, 0])
    ax_lr.plot(phases, learning_rates, marker='o', linestyle='-', color='blue')
    ax_lr.set_title("Learning Rate", fontsize=10)
    ax_lr.set_xlabel("Phase", fontsize=9)
    ax_lr.set_ylabel("LR", fontsize=9)
    ax_lr.set_xticks(phases)
    
    ax_usage = fig.add_subplot(gs_bot[0, 1])
    ax_usage.plot(phases, sample_usage, marker='o', linestyle='-', color='orange')
    ax_usage.set_title("Sample Usage", fontsize=10)
    ax_usage.set_xlabel("Phase", fontsize=9)
    ax_usage.set_ylabel("Usage", fontsize=9)
    ax_usage.set_xticks(phases)
    
    ax_mix = fig.add_subplot(gs_bot[0, 2])
    bar_width = 0.6
    for i in range(num_phases):
        ratio = mixing_ratios[i]
        ax_mix.bar(i, ratio[0], color='green', width=bar_width, label='Easy' if i==0 else "")
        ax_mix.bar(i, ratio[1], bottom=ratio[0], color='orange', width=bar_width, label='Med' if i==0 else "")
        ax_mix.bar(i, ratio[2], bottom=ratio[0]+ratio[1], color='red', width=bar_width, label='Hard' if i==0 else "")
    ax_mix.set_xticks(np.arange(num_phases))
    ax_mix.set_xticklabels([f"P{p}" for p in phases])
    ax_mix.set_title("Mixing Ratios", fontsize=10)
    ax_mix.set_xlabel("Phase", fontsize=9)
    ax_mix.set_ylabel("Ratio", fontsize=9)
    ax_mix.legend(fontsize=8)
    
    ax_reward = fig.add_subplot(gs_bot[0, 3])
    ax_reward.plot(phases, adjusted_rewards, marker='o', linestyle='-', color='black')
    ax_reward.set_title("Reward", fontsize=10)
    ax_reward.set_xlabel("Phase", fontsize=9)
    ax_reward.set_ylabel("Reward", fontsize=9)
    ax_reward.set_xticks(phases)
    
    plt.tight_layout()
    fig_filename = os.path.join(output_dir, f"{group_name}_episode_{episode['index']}_detailed.png")
    plt.savefig(fig_filename)
    plt.close()
    print(f"Saved detailed figure for {group_name} episode {episode['index']} to {fig_filename}")

# ----- Main Training and Evaluation -----

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config", "-c",
        default="config.yaml",
        help="path to your config YAML"
    )
    args   = parser.parse_args()
    config = load_config(args.config)

    # config = load_config("config.yaml")

    set_seed(42)
    
    # Create a separate directory for checkpoints/plots.
    results_dir = os.path.join("results", "off_policy")
    os.makedirs(results_dir, exist_ok=True)
    
    # Load the evolutionary dataset.
    dataset_path = config["paths"]["pretrain_path"]
    data = np.load(dataset_path)
    states = data["states"]
    actions = data["actions"]
    rewards = data["rewards"]
    next_states = data["next_states"]
    dones = data["dones"]

    # Populate the replay buffer.
    if config["rl"].get("per_enabled", False):
        replay_buffer = PERBuffer(
            config["rl"]["buffer_size"],
            alpha=config["rl"].get("per_alpha", 0.6),
            beta=config["rl"].get("per_beta", 0.4),
            epsilon=config["rl"].get("per_epsilon", 1e-6),
            per_type=config["rl"].get("per_type", "proportional")
        )
    else:
        replay_buffer = ReplayBuffer(config["rl"]["buffer_size"])

    if config["rl"].get("seed_replay_buffer", False):
        for i in tqdm(range(len(states)), desc="Seeding Replay Buffer"):
            replay_buffer.push(states[i], actions[i], rewards[i], next_states[i], dones[i])
        print(f"Seeded {len(replay_buffer)} transitions into the replay buffer.")

    print(f"Loaded {len(replay_buffer)} transitions into the replay buffer.")

    env = CurriculumEnv(config)
    obs_dim = len(env.reset())
    action_dim = 5
    agent = DDPGAgent(obs_dim, action_dim, config)

    probe_batch_size = 256
    probe_indices    = np.random.choice(len(states), size=probe_batch_size, replace=False)
    variance_states  = states[probe_indices]
    variance_states_tensor = torch.FloatTensor(variance_states).to(agent.device)
    
    if config["rl"].get("use_behavioral_cloning", False):
        print("Starting BC pretrain with real EA data…")

        # 1) Load your saved EA transitions
        ea = np.load(dataset_path)
        ea_states  = ea["states"]   # shape (N, state_dim)
        ea_actions = ea["actions"]  # shape (N, action_dim)
        ea_rewards = ea["rewards"]  # shape (N,)

        # 2) Precompute sorted indices by reward
        sorted_idx = np.argsort(ea_rewards)  # ascending
        N = len(ea_rewards)

        # 3) Pick positives/negatives based on config
        sel = config["rl"].get("bc_trajectory_selection", "top_k")
        if sel == "top_k":
            k = config["rl"].get("bc_top_k", config["rl"]["batch_size"])
            pos_pool = sorted_idx[-k:]     # ← rename to pos_pool
            neg_pool = sorted_idx[:k]      # ← rename to neg_pool
        elif sel == "threshold":
            thr = config["rl"].get("bc_reward_threshold", np.median(ea_rewards))
            pos_pool = np.where(ea_rewards >= thr)[0]
            neg_pool = np.where(ea_rewards <  thr)[0]
        elif sel == "percentile":
            p = config["rl"].get("bc_percentile", 80)
            high_thr = np.percentile(ea_rewards, p)
            low_thr  = np.percentile(ea_rewards, 100 - p)
            pos_pool = np.where(ea_rewards >= high_thr)[0]
            neg_pool = np.where(ea_rewards <= low_thr)[0]
        else:
            raise ValueError(f"Unknown bc_trajectory_selection: {sel}")

        # 4) Loss function & scheduler
        bc_loss_fn = nn.SmoothL1Loss()
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            agent.actor_optimizer,
            mode='min',
            factor=0.5,
            patience=1000,
            verbose=True
        )
        triplet_fn = (
            nn.TripletMarginLoss(margin=config["rl"].get("triplet_margin", 0.2))
            if config["rl"].get("use_triplet_loss", False) else None
        )

        # 5) BC loop
        iters = config["rl"].get("pretrain_bc_iters", 1000)
        batch = config["rl"]["batch_size"]
        bc_losses = []
        for _ in trange(iters, desc="BC Pretrain"):
            # --- sample anchor transitions uniformly from EA ---
            anc_idx = np.random.randint(0, N, size=batch)
            s_anc   = torch.FloatTensor(ea_states[anc_idx]).to(agent.device)
            a_anc   = torch.FloatTensor(ea_actions[anc_idx]).to(agent.device)

            # --- sample positives & negatives from your pools ---
            if triplet_fn:
                pos_batch = np.random.choice(pos_pool, size=batch, replace=True)
                neg_batch = np.random.choice(neg_pool, size=batch, replace=True)
                a_pos = torch.FloatTensor(ea_actions[pos_batch]).to(agent.device)
                a_neg = torch.FloatTensor(ea_actions[neg_batch]).to(agent.device)

            # --- forward & losses ---
            pred = agent.actor(s_anc)
            loss = bc_loss_fn(pred, a_anc)
            if triplet_fn:
                loss = loss + triplet_fn(pred, a_pos, a_neg)

            # --- backprop actor only ---
            agent.actor_optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(agent.actor.parameters(), max_norm=1.0)
            agent.actor_optimizer.step()
            scheduler.step(loss)

            bc_losses.append(loss.item())

        bc_actor_path = os.path.join(results_dir, "actor_after_bc.pth")
        torch.save(agent.actor.state_dict(), bc_actor_path)
        print(f"Saved actor (post‑BC) → {bc_actor_path}")

        # 1) BC loss progression
        plt.figure()
        plt.plot(bc_losses)
        plt.title("Behavior Cloning Loss Progression")
        plt.xlabel("BC Iteration")
        plt.ylabel("Loss")
        plt.grid(True)
        plt.savefig(os.path.join(results_dir, "bc_loss_progression.png"))
        plt.close()

        # 2) Mixing‐ratio distributions: EA vs. actor predictions
        #    ea_actions is your loaded array; need to compute actor's outputs on same states
        ea_tensor = torch.FloatTensor(ea_states).to(agent.device)
        with torch.no_grad():
            pred_actions = agent.actor(ea_tensor).cpu().numpy()

        for idx, name in zip([1,2,3], ["Easy","Med","Hard"]):
            plt.figure()
            plt.hist(ea_actions[:, idx], bins=30, alpha=0.5, label="EA "+name, density=True)
            plt.hist(pred_actions[:, idx], bins=30, alpha=0.5, label="Actor "+name, density=True)
            plt.title(f"Mixing Ratio – {name}")
            plt.xlabel("Ratio Value")
            plt.ylabel("Density")
            plt.legend()
            plt.savefig(os.path.join(results_dir, f"bc_mixratio_{name.lower()}.png"))
            plt.close()

    if config["rl"].get("pretrain_critic_offpolicy", True):
        print("Pretraining critic off‑policy…")
        pre_iters = config["rl"].get("pretrain_critic_iters", 10000)
        critic1_pre_losses = []
        critic2_pre_losses = []
        for _ in trange(pre_iters, desc="Critic Pretrain"):
            if len(replay_buffer) >= config["rl"]["batch_size"]:
                # only update critic
                metrics = agent.critic_update_only(replay_buffer, config["rl"]["batch_size"])
                critic1_pre_losses.append(metrics["critic1_loss"])
                critic2_pre_losses.append(metrics["critic2_loss"])
        
         # Save both critics
        critic1_pre_path = os.path.join(results_dir, "critic1_after_pretrain.pth")
        critic2_pre_path = os.path.join(results_dir, "critic2_after_pretrain.pth")
        torch.save(agent.critic1.state_dict(), critic1_pre_path)
        torch.save(agent.critic2.state_dict(), critic2_pre_path)
        print(f"Saved critic1 → {critic1_pre_path}")
        print(f"Saved critic2 → {critic2_pre_path}")
        
        # Plot both losses
        plt.figure()
        plt.plot(critic1_pre_losses, label="Critic1 Loss")
        plt.plot(critic2_pre_losses, label="Critic2 Loss")
        plt.title("Critic Off‑Policy Pretrain Losses")
        plt.xlabel("Iteration")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(results_dir, "critic_pretrain_losses.png"))
        plt.close()

    # Prepare for checkpointing.
    num_updates = config["rl"].get("off_policy_updates", int(1e6))
    checkpoint_interval = int(num_updates * 0.2)
    if checkpoint_interval == 0:
        checkpoint_interval = 1

    # Lists for tracking metrics over training.
    actor_losses = []
    critic1_losses = []
    critic2_losses = []
    reward_progress = []
    eval_updates = []
    num_bins = config["observation"]["num_bins"]
    action_var_history = []   
    update_steps      = []

    # Training loop.
    for update in trange(num_updates, desc="Off‑policy Training"):
        if len(replay_buffer) >= config["rl"]["batch_size"]:
            metrics = agent.update(replay_buffer, config["rl"]["batch_size"])
            if metrics is not None:
                actor_losses.append(metrics["actor_loss"])
                critic1_losses.append(metrics["critic1_loss"])
                critic2_losses.append(metrics["critic2_loss"])
        with torch.no_grad():
            pred_actions = agent.actor(variance_states_tensor).cpu().numpy()  # (256,5)
        var = np.var(pred_actions, axis=0)  # length‑5 vector
        action_var_history.append(var)
        update_steps.append(update)
        
        # Every checkpoint_interval (5% of training), run a full evaluation episode
        # and save all plots and model checkpoint.
        if update % checkpoint_interval == 0:
            eval_states = []
            eval_actions = []
            eval_rewards = []
            obs_eval = env.reset()
            done = False
            while not done:
                action_eval = agent.select_action(obs_eval, noise_enable=False)
                eval_states.append(obs_eval)
                eval_actions.append(action_eval)
                obs_eval, reward, done = env.step(action_eval)
                eval_rewards.append(reward)
            total_reward = sum(eval_rewards)
            reward_progress.append(total_reward)
            eval_updates.append(update)
            print(f"Checkpoint at update {update}/{num_updates} - Full episode evaluation reward: {total_reward}")
            
            # Build an evaluation episode dictionary.
            eval_episode = {
                "index": update,
                "states": np.array(eval_states),
                "actions": np.array(eval_actions),
                "rewards": np.array(eval_rewards),
                "total_reward": total_reward,
                "episode_length": len(eval_states)
            }
            
            # Save detailed episode plot.
            plot_episode_figure(eval_episode, f"eval_{update}", num_bins, results_dir)
            
            # Save loss plots.
            fig_loss, (ax_actor, ax_critic) = plt.subplots(2, 1, figsize=(8, 10))
            ax_actor.plot(actor_losses, color='blue')
            ax_actor.set_title("Actor Loss Progression")
            ax_actor.set_xlabel("Update Steps")
            ax_actor.set_ylabel("Loss")
            ax_critic.plot(critic1_losses, label="Critic1")
            ax_critic.plot(critic2_losses, label="Critic2")
            ax_critic.set_title("Critic Loss Progression")
            ax_critic.set_xlabel("Update Steps")
            ax_critic.set_ylabel("Loss")
            ax_critic.legend()
            checkpoint_loss_path = os.path.join(results_dir, f"training_losses_{update}.png")
            fig_loss.tight_layout()
            fig_loss.savefig(checkpoint_loss_path)
            plt.close(fig_loss)
            
            # Save reward progression plot.
            plt.figure()
            plt.plot(eval_updates, reward_progress, marker='o')
            plt.xlabel("Update Steps")
            plt.ylabel("Reward")
            plt.title("Periodic Reward Evaluation")
            checkpoint_reward_path = os.path.join(results_dir, f"reward_progress_{update}.png")
            plt.savefig(checkpoint_reward_path)
            plt.close()
            
            # Save model checkpoint.
            actor_ckpt = os.path.join(results_dir, f"off_policy_actor_{update}.pth")
            torch.save(agent.actor.state_dict(), actor_ckpt)
            print(f"Saved actor → {actor_ckpt}")
            crit1_ckpt = os.path.join(results_dir, f"off_policy_critic1_{update}.pth")
            crit2_ckpt = os.path.join(results_dir, f"off_policy_critic2_{update}.pth")
            torch.save(agent.critic1.state_dict(), crit1_ckpt)
            torch.save(agent.critic2.state_dict(), crit2_ckpt)
            print(f"Saved critic1 → {crit1_ckpt}")
            print(f"Saved critic2 → {crit2_ckpt}")

            # plot action‐component variances
            action_var_array = np.vstack(action_var_history)  # shape (steps,5)
            names = ["learning_rate","mix_easy","mix_med","mix_hard","sample_usage"]

            # 5‐panel subplot, one row per action component
            fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(8,12), sharex=True)
            for idx, name in enumerate(names):
                axs[idx].plot(update_steps, action_var_array[:,idx])
                axs[idx].set_ylabel("Var")
                axs[idx].set_title(name)
                axs[idx].grid(True)

            # only bottom panel needs an x‐label
            axs[-1].set_xlabel("Update Step")

            plt.tight_layout()
            plt.savefig(os.path.join(results_dir, "action_variance.png"))
            plt.close()
                
    # After training, run one final full evaluation episode.
    eval_states = []
    eval_actions = []
    eval_rewards = []
    obs_eval = env.reset()
    done = False
    while not done:
        action_eval = agent.select_action(obs_eval, noise_enable=False)
        eval_states.append(obs_eval)
        eval_actions.append(action_eval)
        obs_eval, reward, done = env.step(action_eval)
        eval_rewards.append(reward)
    total_reward = sum(eval_rewards)
    print(f"Final evaluation episode total reward: {total_reward}")
    
    eval_episode = {
        "index": num_updates,
        "states": np.array(eval_states),
        "actions": np.array(eval_actions),
        "rewards": np.array(eval_rewards),
        "total_reward": total_reward,
        "episode_length": len(eval_states)
    }
    plot_episode_figure(eval_episode, "final_eval", num_bins, results_dir)
    print(f"Saved final evaluation episode plot in {results_dir}")
    
    # Final actor save
    final_actor = os.path.join(results_dir, "off_policy_actor_model_final.pth")
    torch.save(agent.actor.state_dict(), final_actor)

    final_critic1 = os.path.join(results_dir, "off_policy_critic1_model_final.pth")
    final_critic2 = os.path.join(results_dir, "off_policy_critic2_model_final.pth")

    torch.save(agent.critic1.state_dict(), final_critic1)
    torch.save(agent.critic2.state_dict(), final_critic2)

    print(f"Saved final actor → {final_actor}")
    print(f"Saved final critic 1 → {final_critic1}")
    print(f"Saved final critic 2 → {final_critic2}")
    
    # Plot final actor and critic loss progression on separate subplots.
    fig_loss, (ax_actor, ax_critic) = plt.subplots(2, 1, figsize=(8, 10))
    ax_actor.plot(actor_losses, color='blue')
    ax_actor.set_title("Actor Loss Progression")
    ax_actor.set_xlabel("Update Steps")
    ax_actor.set_ylabel("Loss")
    ax_critic.plot(critic1_losses, color='red', label="Critic 1")
    ax_critic.plot(critic2_losses, color='green', label="Critic 2")
    ax_critic.set_title("Critic Loss Progression")
    ax_critic.set_xlabel("Update Steps")
    ax_critic.set_ylabel("Loss")
    ax_critic.legend()
    final_loss_plot = os.path.join(results_dir, "training_losses_final.png")
    fig_loss.tight_layout()
    fig_loss.savefig(final_loss_plot)
    plt.close(fig_loss)
    print(f"Final training loss plot saved to {final_loss_plot}")
    
    # Plot final reward progression.
    plt.figure()
    plt.plot(eval_updates, reward_progress, marker='o')
    plt.xlabel("Update Steps")
    plt.ylabel("Reward")
    plt.title("Periodic Reward Evaluation")
    final_reward_plot = os.path.join(results_dir, "reward_progress_final.png")
    plt.savefig(final_reward_plot)
    plt.close()
    print(f"Final reward progression plot saved to {final_reward_plot}")

if __name__ == "__main__":
    main()










