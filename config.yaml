# File: config.yaml

# Compute device for training. Change to "cpu" if no GPU is available.
device: "cuda:1"

# Global batch size for data loaders (burn‑in, curriculum phases, etc.)
batch_size: 512

fractions:
  easy: 0.9      # Proportion of samples considered “easy”
  medium: 0.075  # Proportion of samples considered “medium”

curriculum:
  train_samples_max: 240000
  learning_rate_range: [0.001, 0.01]  # narrower LR range for smoother updates
  max_phases: 3
  min_sample_usage: 100

rl:
  # Evolutionary algorithm settings 
  ea_pop_size:      100
  ea_generations:   10
  ea_top_k:         10
  ea_mutation_rate: 0.1

  # Off‑policy pretrain (reduced iterations for faster turnaround)
  pretrain_bc_iters:       1000000
  pretrain_critic_iters:   1000000
  off_policy_updates:      1000000
  
  # On‑policy training (tuned)
  on_policy_episodes:           100000    # fewer, more meaningful episodes
  on_policy_updates_per_episode: 1     # extra updates per episode for stability
  batch_size:                    256    # smaller batches for on‑policy updates
  probe_from_ea:                 true
  probe_batch_size:              128

  # Learning rates for actor & critic
  actor_lr: 0.0003
  critic_lr: 0.001

  # Discount & target‐update smoothing
  gamma: 0.95
  tau:   0.01

  # Exploration noise
  exploration_noise: 0.2

  # Prioritized replay: disabled for on‑policy simplicity
  per_enabled: false
  per_type:    "proportional"
  per_alpha:   0.6
  per_beta:    0.4
  per_epsilon: 1e-6

  # Behavioral cloning settings (unchanged)s
  use_behavioral_cloning: true
  bc_trajectory_selection: "top_k"
  bc_top_k:               250
  bc_reward_threshold:    0.5
  bc_percentile:          80
  use_triplet_loss:       true
  triplet_margin:         0.2
  pretrain_critic_offpolicy: true

  # Replay buffer size
  seed_replay_buffer: true
  buffer_size: 1000000

observation:
  num_bins: 64  # Number of bins for each loss histogram

paths:
  save_path:                  "results/curriculum_rl"
  data_path:                  "./data"
  pretrain_path:              "results/curriculum_rl/evolutionary_dataset.npz"
  off_policy_actor_model:     "results/off_policy/off_policy_actor_model_final.pth"
  off_policy_critic_model:    "results/off_policy/off_policy_critic_model_final.pth"
  on_policy_dir:              "results/on_policy"
  on_policy_actor_model:      "results/off_policy/off_policy_actor_model_final.pth"
  on_policy_critic_model:     "results/off_policy/off_policy_critic_model_final.pth"

# Global random seed for reproducibility
seed: 42


compare_models:
  Off Policy Start:
    actor:  "results/off_policy/off_policy_actor_0.pth"
    critic: "results/off_policy/off_policy_critic_0.pth"
  Off Policy End:
    actor:  "results/off_policy/off_policy_actor_model_final.pth"
    critic: "results/off_policy/off_policy_critic_model_final.pth"
  On Policy Start:
    actor:  "results/on_policy/actor_ep10.pth"
    critic: "results/on_policy/critic_ep10.pth"
  

